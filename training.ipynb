{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/users/j/s/jstonge1/miniconda3/envs/llama_env/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/users/j/s/jstonge1/miniconda3/envs/llama_env/lib/python3.9/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "/users/j/s/jstonge1/miniconda3/envs/llama_env/lib/python3.9/site-packages/transformers/utils/generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "/users/j/s/jstonge1/miniconda3/envs/llama_env/lib/python3.9/site-packages/transformers/utils/generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n"
     ]
    }
   ],
   "source": [
    "# Set up latest torch instance with Python=3.9\n",
    "import torch\n",
    "from transformers import LlamaForCausalLM, LlamaTokenizer\n",
    "from datasets import load_dataset\n",
    "from pathlib import Path\n",
    "import json\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir = Path()\n",
    "model_id = root_dir / \"weights\" / \"weights\" / \"13Bf_hf\" # location for model directory, must be in torch format\n",
    "tokenizer = LlamaTokenizer.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 6/6 [00:35<00:00,  5.85s/it]\n"
     ]
    }
   ],
   "source": [
    "model = LlamaForCausalLM.from_pretrained(model_id, device_map='auto', torch_dtype=torch.float16) # device_map='auto' will use GPU if available"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sys Prompt and Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys_prompt = \"\"\"Pretend you are an observant Assistant, a helpful bot that takes course catalogue data and returns clean JSON object list. If available, the JSON objects should contain for each course in the text a \"Number\", \"Title\", \"Description\", \"Prerequisite\", and \"Credits\". There can be multiple Course Numbers and multiple Prerequisites, include all of them.  If the text is not a course description, return \"[]\".\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = load_dataset('json', data_files='data/train.json', split='train')  \n",
    "\n",
    "# split train into train and validation\n",
    "train_dataset = train_dataset.train_test_split(test_size=0.09, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'filename': '2008_2009_211',\n",
       " 'output': '[\\n    {\\n        \"Number\": [\\n            132\\n        ],\\n        \"Title\": \"Post Bop Ensemble\",\\n        \"Description\": \"A small jazz group (rhythm section plus two to four horns) specializing in post-1950s repertoire (Wayne Shorter, Chick Corea, etc.) as well as original compositions.\",\\n        \"Prerequisites\": \"audition\",\\n        \"Credits\": \"1\"\\n    },\\n    {\\n        \"Number\": [\\n            133\\n        ],\\n        \"Title\": \"Applied Lessons\",\\n        \"Description\": \"Private instruction in an instru - ment or voice for music minors. Subject to avail - ability of staff. Lab fee required. May be repeated for credit.\",\\n        \"Prerequisites\": \"successful completion of Level I Examination.\",\\n        \"Credits\": \"1 OR 2\"\\n    },\\n    {\\n        \"Number\": [\\n            149\\n        ],\\n        \"Title\": \"Soph RecitalPerformance\",\\n        \"Description\": \"Sem B.M. Candi-dates only\",\\n        \"Prerequisites\": \"\",\\n        \"Credits\": \"1\"\\n    },\\n    {\\n        \"Number\": [\\n            134\\n        ],\\n        \"Title\": \"Applied Lessons\",\\n        \"Description\": \"Private i nstruction in an instru- ment or voice for music majors. Lab fee required. Juried examinations generally every semester of study. May be repeated for credit.\",\\n        \"Prerequisites\": \"successful completion of Level II Examination.\",\\n        \"Credits\": \"1 OR 2\"\\n    },\\n    {\\n        \"Number\": [\\n            150\\n        ],\\n        \"Title\": \"Junior Recital B.M\",\\n        \"Description\": \"Candidates only\",\\n        \"Prerequisites\": \"\",\\n        \"Credits\": \"1\"\\n    },\\n    {\\n        \"Number\": [\\n            154\\n        ],\\n        \"Title\": \"Harmony and Form Lab III\",\\n        \"Description\": \"Intensive study of solfege (music reading), chromatic harmony at the keyboard, dictation and open-score reading\",\\n        \"Prerequisites\": \"56, or instructors permission\",\\n        \"Credits\": \"1\"\\n    },\\n    {\\n        \"Number\": [\\n            156\\n        ],\\n        \"Title\": \"Harmony and Form Lab IV\",\\n        \"Description\": \"Intensive study of solfege (music reading), extended tonality and atonality at the keyboard, dictation, and open-score reading.\",\\n        \"Prerequisites\": \"154, or instructors permission.\",\\n        \"Credits\": \"1\"\\n    },\\n    {\\n        \"Number\": [\\n            157\\n        ],\\n        \"Title\": \"Composition\",\\n        \"Description\": \"Preliminary studies in free com - position and the mechanics of score preparation; composition of an extended work for one to five instruments or voices.\",\\n        \"Prerequisites\": \"MU 109 and MU 110, or instructors permission.\",\\n        \"Credits\": \"3\"\\n    },\\n    {\\n        \"Number\": [\\n            159\\n        ],\\n        \"Title\": \"TheoryPrac Jazz Improv I\",\\n        \"Description\": \"Basic repertory, idiomatic usage, aural skills, theoretical constructs, and strategies for the jazz improvisor.\",\\n        \"Prerequisites\": \"intermediate instrumental skill, ability to read music, previous study of traditional music theory .\",\\n        \"Credits\": \"3\"\\n    },\\n    {\\n        \"Number\": [\\n            176\\n        ],\\n        \"Title\": \"Music for Elem Teachers\",\\n        \"Description\": \"Development of musi- cal skills, understandings, and attitudes for teach-ing music in the elementary classroom\",\\n        \"Prerequisites\": \"Sophomore standing in elementary education, and early childhood majors only; or acceptance into licensure program\",\\n        \"Credits\": \"3\"\\n    },\\n    {\\n        \"Number\": [\\n            181\\n        ],\\n        \"Title\": \"Conducting\",\\n        \"Description\": \"Baton technique, score reading, and laboratory practice. Preparation and performance of selected scores, including rehearsal procedures.\",\\n        \"Prerequisites\": \"MU 154 and MU 209.\",\\n        \"Credits\": \"3\"\\n    },\\n    {\\n        \"Number\": [\\n            195\\n        ],\\n        \"Title\": \"Special T opics\",\\n        \"Description\": \"Courses on topics beyond the scope of existing departmental offerings. See schedule of courses for specific titles.\",\\n        \"Prerequisites\": \"MU 109 and MU 110. Majorsminors or instructors permission.\",\\n        \"Credits\": \"1 - 3\"\\n    },\\n    {\\n        \"Number\": [\\n            196\\n        ],\\n        \"Title\": \"Special T opics\",\\n        \"Description\": \"Courses on topics beyond the scope of existing departmental offerings. See schedule of courses for specific titles.\",\\n        \"Prerequisites\": \"MU 109 and MU 110. Majorsminors, or instructors permission.\",\\n        \"Credits\": \"3\"\\n    },\\n    {\\n        \"Number\": [\\n            197\\n        ],\\n        \"Title\": \"Readings and Research\",\\n        \"Description\": \"Supervised independent study in music. Inter-disciplinary topics are encour - aged.\",\\n        \"Prerequisites\": \"Departmental permission.\",\\n        \"Credits\": \"1 - 6\"\\n    }\\n]',\n",
       " 'input': ' 1 - 6 \\n132 Post Bop Ensemble  A\\tsmall\\tjazz\\tgroup\\t(rhythm\\t\\nsection\\tplus\\ttwo\\tto\\tfour\\thorns)\\tspecializing\\tin\\tpost-1950’s\\trepertoire\\t(Wayne\\tShorter,\\tChick\\tCorea,\\tetc.)\\tas well as original compositions. Prerequisite : audition \\nCredits: 1 \\n133 Applied Lessons  Private instruction in an instru -\\nment or voice for music minors. Subject to avail -\\nability\\tof\\tstaff.\\tLab\\tfee\\trequired.\\tMay\\tbe\\trepeated\\tfor\\t\\ncredit. Prerequisite : successful completion of  Level I \\nExamination. Credits: 1 OR 2 134 Applied Lessons  Private i nstruction in an instru-\\nment\\tor\\tvoice\\tfor\\tmusic\\tmajors.\\tLab\\tfee \\trequired. \\t\\nJuried\\texaminations\\tgenerally\\tevery\\tsemester\\tof \\tstudy.\\t\\nMay be repeated for credit. Prerequisite: successful \\ncompletion of  Level II Examination. Credits: 1 OR 2 \\n149 Soph Recital/Performance Sem B.M.\\tCandi-dates only . Credits: 1 \\n150 Junior Recital  B.M.\\tCandidates\\tonly.\\tCredits: 1 \\n154 Harmony and Form Lab III  Intensive study of  \\nsolfege\\t(music\\treading),\\tchromatic\\tharmony\\tat\\tthe\\tkeyboard,\\tdictation\\tand\\topen-score\\treading.\\tPrerequi-\\nsite:\\t56,\\tor\\tinstructor’s\\tpermission.\\t.\\t Credits: 1 \\n156 Harmony and Form Lab IV  Intensive study \\nof\\tsolfege\\t(music\\treading),\\textended\\ttonality\\tand\\t\\natonality\\tat\\tthe\\tkeyboard,\\tdictation,\\tand\\topen-score\\treading. Prerequisite :\\t154,\\tor\\tinstructor’s\\tpermission.\\t\\nCredits: 1 \\n157 Composition  Preliminary studies in free com -\\nposition\\tand\\tthe\\tmechanics\\tof \\tscore\\tpreparation;\\t\\ncomposition\\tof \\tan\\textended\\twork\\tfor\\tone\\tto\\tfive\\t\\ninstruments or voices. Prerequisite : MU 109 and MU \\n110,\\tor\\tinstructor’s\\tpermission.\\t Credits: 3 \\n159 Theory/Prac Jazz Improv I Basic\\trepertory,\\t\\nidiomatic\\tusage,\\taural\\tskills,\\ttheoretical\\tconstructs,\\t\\nand strategies for the jazz improvisor. Prerequisite : \\nintermediate\\tinstrumental\\tskill,\\tability\\tto\\tread\\tmusic,\\t\\nprevious study of  traditional music theory . Credits: 3 \\n176 Music for Elem Teachers Development of  musi-\\ncal\\tskills,\\tunderstandings,\\tand\\tattitudes\\tfor\\tteach-ing music in the elementary classroom. Prerequisite : \\nSophomore\\tstanding\\tin\\telementary\\teducation,\\tand\\tearly\\tchildhood\\tmajors\\tonly;\\tor\\tacceptance\\tinto\\tlicensure program. Credits: 3 \\n181 Conducting Baton\\ttechnique,\\tscore\\treading,\\tand\\t\\nlaboratory practice. Preparation and performance \\nof\\tselected\\tscores,\\tincluding\\trehearsal\\tprocedures.\\t\\nPrerequisite : MU 154 and MU 209. Credits: 3 \\n195 Special T opics Courses on topics beyond the scope of  existing departmental offerings. See schedule of  courses\\tfor\\tspecific\\ttitles.\\tPrerequisite : MU 109 and \\nMU\\t110.\\tMajors/minors\\tor\\tinstructor’s\\tpermission.\\t\\nCredits: 1 - 3 \\n196 Special T opics Courses on topics beyond the scope \\nof  existing departmental offerings. See schedule of  \\ncourses\\tfor\\tspecific\\ttitles.\\tPrerequisite : MU 109 and \\nMU\\t110.\\tMajors/minors,\\tor\\tinstructor’s\\tpermission.\\t\\nCredits: 3 \\n197 Readings and Research Supervised independent \\nstudy in music. Inter-disciplinary topics are encour -\\naged. Pre/co-requisites : Departmental permission. \\nCredits: 1 - 6 '}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The correct json format-\n",
    "train_dataset['train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def formatting_func(example):\n",
    "    text = f\"\"\"<s>[INST] <<SYS>> {sys_prompt} <</SYS>>\n",
    "\n",
    "```{example['input']}``` \n",
    "Make sure to include prerequisites and exclude any \n",
    "non-course information. [/INST] {example['output']}\n",
    "\"\"\"\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s>[INST] <<SYS>> Pretend you are an observant Assistant, a helpful bot that takes course catalogue data and returns clean JSON object list. If available, the JSON objects should contain for each course in the text a \"Number\", \"Title\", \"Description\", \"Prerequisite\", and \"Credits\". There can be multiple Course Numbers and multiple Prerequisites, include all of them.  If the text is not a course description, return \"[]\". <</SYS>>\n",
      "\n",
      "``` 1 - 6 \n",
      "132 Post Bop Ensemble  A\tsmall\tjazz\tgroup\t(rhythm\t\n",
      "section\tplus\ttwo\tto\tfour\thorns)\tspecializing\tin\tpost-1950’s\trepertoire\t(Wayne\tShorter,\tChick\tCorea,\tetc.)\tas well as original compositions. Prerequisite : audition \n",
      "Credits: 1 \n",
      "133 Applied Lessons  Private instruction in an instru -\n",
      "ment or voice for music minors. Subject to avail -\n",
      "ability\tof\tstaff.\tLab\tfee\trequired.\tMay\tbe\trepeated\tfor\t\n",
      "credit. Prerequisite : successful completion of  Level I \n",
      "Examination. Credits: 1 OR 2 134 Applied Lessons  Private i nstruction in an instru-\n",
      "ment\tor\tvoice\tfor\tmusic\tmajors.\tLab\tfee \trequired. \t\n",
      "Juried\texaminations\tgenerally\tevery\tsemester\tof \tstudy.\t\n",
      "May be repeated for credit. Prerequisite: successful \n",
      "completion of  Level II Examination. Credits: 1 OR 2 \n",
      "149 Soph Recital/Performance Sem B.M.\tCandi-dates only . Credits: 1 \n",
      "150 Junior Recital  B.M.\tCandidates\tonly.\tCredits: 1 \n",
      "154 Harmony and Form Lab III  Intensive study of  \n",
      "solfege\t(music\treading),\tchromatic\tharmony\tat\tthe\tkeyboard,\tdictation\tand\topen-score\treading.\tPrerequi-\n",
      "site:\t56,\tor\tinstructor’s\tpermission.\t.\t Credits: 1 \n",
      "156 Harmony and Form Lab IV  Intensive study \n",
      "of\tsolfege\t(music\treading),\textended\ttonality\tand\t\n",
      "atonality\tat\tthe\tkeyboard,\tdictation,\tand\topen-score\treading. Prerequisite :\t154,\tor\tinstructor’s\tpermission.\t\n",
      "Credits: 1 \n",
      "157 Composition  Preliminary studies in free com -\n",
      "position\tand\tthe\tmechanics\tof \tscore\tpreparation;\t\n",
      "composition\tof \tan\textended\twork\tfor\tone\tto\tfive\t\n",
      "instruments or voices. Prerequisite : MU 109 and MU \n",
      "110,\tor\tinstructor’s\tpermission.\t Credits: 3 \n",
      "159 Theory/Prac Jazz Improv I Basic\trepertory,\t\n",
      "idiomatic\tusage,\taural\tskills,\ttheoretical\tconstructs,\t\n",
      "and strategies for the jazz improvisor. Prerequisite : \n",
      "intermediate\tinstrumental\tskill,\tability\tto\tread\tmusic,\t\n",
      "previous study of  traditional music theory . Credits: 3 \n",
      "176 Music for Elem Teachers Development of  musi-\n",
      "cal\tskills,\tunderstandings,\tand\tattitudes\tfor\tteach-ing music in the elementary classroom. Prerequisite : \n",
      "Sophomore\tstanding\tin\telementary\teducation,\tand\tearly\tchildhood\tmajors\tonly;\tor\tacceptance\tinto\tlicensure program. Credits: 3 \n",
      "181 Conducting Baton\ttechnique,\tscore\treading,\tand\t\n",
      "laboratory practice. Preparation and performance \n",
      "of\tselected\tscores,\tincluding\trehearsal\tprocedures.\t\n",
      "Prerequisite : MU 154 and MU 209. Credits: 3 \n",
      "195 Special T opics Courses on topics beyond the scope of  existing departmental offerings. See schedule of  courses\tfor\tspecific\ttitles.\tPrerequisite : MU 109 and \n",
      "MU\t110.\tMajors/minors\tor\tinstructor’s\tpermission.\t\n",
      "Credits: 1 - 3 \n",
      "196 Special T opics Courses on topics beyond the scope \n",
      "of  existing departmental offerings. See schedule of  \n",
      "courses\tfor\tspecific\ttitles.\tPrerequisite : MU 109 and \n",
      "MU\t110.\tMajors/minors,\tor\tinstructor’s\tpermission.\t\n",
      "Credits: 3 \n",
      "197 Readings and Research Supervised independent \n",
      "study in music. Inter-disciplinary topics are encour -\n",
      "aged. Pre/co-requisites : Departmental permission. \n",
      "Credits: 1 - 6 ``` \n",
      "Make sure to include prerequisites and exclude any \n",
      "non-course information. [/INST] [\n",
      "    {\n",
      "        \"Number\": [\n",
      "            132\n",
      "        ],\n",
      "        \"Title\": \"Post Bop Ensemble\",\n",
      "        \"Description\": \"A small jazz group (rhythm section plus two to four horns) specializing in post-1950s repertoire (Wayne Shorter, Chick Corea, etc.) as well as original compositions.\",\n",
      "        \"Prerequisites\": \"audition\",\n",
      "        \"Credits\": \"1\"\n",
      "    },\n",
      "    {\n",
      "        \"Number\": [\n",
      "            133\n",
      "        ],\n",
      "        \"Title\": \"Applied Lessons\",\n",
      "        \"Description\": \"Private instruction in an instru - ment or voice for music minors. Subject to avail - ability of staff. Lab fee required. May be repeated for credit.\",\n",
      "        \"Prerequisites\": \"successful completion of Level I Examination.\",\n",
      "        \"Credits\": \"1 OR 2\"\n",
      "    },\n",
      "    {\n",
      "        \"Number\": [\n",
      "            149\n",
      "        ],\n",
      "        \"Title\": \"Soph RecitalPerformance\",\n",
      "        \"Description\": \"Sem B.M. Candi-dates only\",\n",
      "        \"Prerequisites\": \"\",\n",
      "        \"Credits\": \"1\"\n",
      "    },\n",
      "    {\n",
      "        \"Number\": [\n",
      "            134\n",
      "        ],\n",
      "        \"Title\": \"Applied Lessons\",\n",
      "        \"Description\": \"Private i nstruction in an instru- ment or voice for music majors. Lab fee required. Juried examinations generally every semester of study. May be repeated for credit.\",\n",
      "        \"Prerequisites\": \"successful completion of Level II Examination.\",\n",
      "        \"Credits\": \"1 OR 2\"\n",
      "    },\n",
      "    {\n",
      "        \"Number\": [\n",
      "            150\n",
      "        ],\n",
      "        \"Title\": \"Junior Recital B.M\",\n",
      "        \"Description\": \"Candidates only\",\n",
      "        \"Prerequisites\": \"\",\n",
      "        \"Credits\": \"1\"\n",
      "    },\n",
      "    {\n",
      "        \"Number\": [\n",
      "            154\n",
      "        ],\n",
      "        \"Title\": \"Harmony and Form Lab III\",\n",
      "        \"Description\": \"Intensive study of solfege (music reading), chromatic harmony at the keyboard, dictation and open-score reading\",\n",
      "        \"Prerequisites\": \"56, or instructors permission\",\n",
      "        \"Credits\": \"1\"\n",
      "    },\n",
      "    {\n",
      "        \"Number\": [\n",
      "            156\n",
      "        ],\n",
      "        \"Title\": \"Harmony and Form Lab IV\",\n",
      "        \"Description\": \"Intensive study of solfege (music reading), extended tonality and atonality at the keyboard, dictation, and open-score reading.\",\n",
      "        \"Prerequisites\": \"154, or instructors permission.\",\n",
      "        \"Credits\": \"1\"\n",
      "    },\n",
      "    {\n",
      "        \"Number\": [\n",
      "            157\n",
      "        ],\n",
      "        \"Title\": \"Composition\",\n",
      "        \"Description\": \"Preliminary studies in free com - position and the mechanics of score preparation; composition of an extended work for one to five instruments or voices.\",\n",
      "        \"Prerequisites\": \"MU 109 and MU 110, or instructors permission.\",\n",
      "        \"Credits\": \"3\"\n",
      "    },\n",
      "    {\n",
      "        \"Number\": [\n",
      "            159\n",
      "        ],\n",
      "        \"Title\": \"TheoryPrac Jazz Improv I\",\n",
      "        \"Description\": \"Basic repertory, idiomatic usage, aural skills, theoretical constructs, and strategies for the jazz improvisor.\",\n",
      "        \"Prerequisites\": \"intermediate instrumental skill, ability to read music, previous study of traditional music theory .\",\n",
      "        \"Credits\": \"3\"\n",
      "    },\n",
      "    {\n",
      "        \"Number\": [\n",
      "            176\n",
      "        ],\n",
      "        \"Title\": \"Music for Elem Teachers\",\n",
      "        \"Description\": \"Development of musi- cal skills, understandings, and attitudes for teach-ing music in the elementary classroom\",\n",
      "        \"Prerequisites\": \"Sophomore standing in elementary education, and early childhood majors only; or acceptance into licensure program\",\n",
      "        \"Credits\": \"3\"\n",
      "    },\n",
      "    {\n",
      "        \"Number\": [\n",
      "            181\n",
      "        ],\n",
      "        \"Title\": \"Conducting\",\n",
      "        \"Description\": \"Baton technique, score reading, and laboratory practice. Preparation and performance of selected scores, including rehearsal procedures.\",\n",
      "        \"Prerequisites\": \"MU 154 and MU 209.\",\n",
      "        \"Credits\": \"3\"\n",
      "    },\n",
      "    {\n",
      "        \"Number\": [\n",
      "            195\n",
      "        ],\n",
      "        \"Title\": \"Special T opics\",\n",
      "        \"Description\": \"Courses on topics beyond the scope of existing departmental offerings. See schedule of courses for specific titles.\",\n",
      "        \"Prerequisites\": \"MU 109 and MU 110. Majorsminors or instructors permission.\",\n",
      "        \"Credits\": \"1 - 3\"\n",
      "    },\n",
      "    {\n",
      "        \"Number\": [\n",
      "            196\n",
      "        ],\n",
      "        \"Title\": \"Special T opics\",\n",
      "        \"Description\": \"Courses on topics beyond the scope of existing departmental offerings. See schedule of courses for specific titles.\",\n",
      "        \"Prerequisites\": \"MU 109 and MU 110. Majorsminors, or instructors permission.\",\n",
      "        \"Credits\": \"3\"\n",
      "    },\n",
      "    {\n",
      "        \"Number\": [\n",
      "            197\n",
      "        ],\n",
      "        \"Title\": \"Readings and Research\",\n",
      "        \"Description\": \"Supervised independent study in music. Inter-disciplinary topics are encour - aged.\",\n",
      "        \"Prerequisites\": \"Departmental permission.\",\n",
      "        \"Credits\": \"1 - 6\"\n",
      "    }\n",
      "]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# formatted data\n",
    "print(formatting_func(train_dataset['train'][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_and_tokenize_prompt(prompt):\n",
    "    return tokenizer(formatting_func(prompt)) # tokenize the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:   0%|          | 0/249 [00:00<?, ? examples/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 249/249 [00:04<00:00, 61.38 examples/s]\n",
      "Map: 100%|██████████| 25/25 [00:00<00:00, 63.45 examples/s]\n"
     ]
    }
   ],
   "source": [
    "tokenized_train_dataset = train_dataset[\"train\"].map(generate_and_tokenize_prompt)\n",
    "tokenized_validate_dataset = train_dataset[\"test\"].map(generate_and_tokenize_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Uncomment if you want to evaluate first ######\n",
    "\n",
    "# eval_prompt = f\"\"\"<<SYS>>{sys_prompt}<</SYS>>\n",
    "\n",
    "# [INST]Text: {tokenized_validate_dataset[10]['input']}[/INST]\n",
    "# \"\"\"\n",
    "\n",
    "# model_input = tokenizer(eval_prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "# model.eval()\n",
    "# with torch.no_grad():\n",
    "#     print(tokenizer.decode(model.generate(**model_input, max_new_tokens=1000)[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.pad_token = tokenizer.eos_token # llama quirk, have to do it\n",
    "model.gradient_checkpointing_enable() # makes the training faster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up Model Training with LoRa Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "'CUDASetup' object has no attribute 'cuda_available'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/users/j/s/jstonge1/miniconda3/envs/llama_env/lib/python3.9/site-packages/bitsandbytes/cextension.py:34: UserWarning: The installed version of bitsandbytes was compiled without GPU support. 8-bit optimizers, 8-bit multiplication, and GPU quantization are unavailable.\n",
      "  warn(\"The installed version of bitsandbytes was compiled without GPU support. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 1,010,892,800 || all params: 14,026,757,120 || trainable%: 7.20688888637433\n"
     ]
    }
   ],
   "source": [
    "model.train() # put the model in training mode\n",
    "\n",
    "def create_peft_config(model):\n",
    "    from peft import (\n",
    "        get_peft_model,\n",
    "        LoraConfig,\n",
    "        TaskType,\n",
    "    )\n",
    "    peft_config = LoraConfig(\n",
    "        task_type=TaskType.CAUSAL_LM,\n",
    "        inference_mode=False,\n",
    "        r=256, # tune this\n",
    "        lora_alpha=512, # and this\n",
    "        lora_dropout=0.05,\n",
    "        # the target modules can also be tuned\n",
    "        target_modules=[\n",
    "        \"q_proj\",\n",
    "        \"k_proj\",\n",
    "        \"v_proj\",\n",
    "        \"o_proj\",\n",
    "        \"gate_proj\",\n",
    "        \"up_proj\",\n",
    "        \"down_proj\",\n",
    "        \"lm_head\",\n",
    "    ]\n",
    "    )\n",
    "    model = get_peft_model(model, peft_config)\n",
    "    model.print_trainable_parameters()\n",
    "    return model, peft_config\n",
    "try:\n",
    "    # create peft config\n",
    "    model, lora_config = create_peft_config(model)\n",
    "except:\n",
    "    model, lora_config = create_peft_config(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainerCallback\n",
    "from contextlib import nullcontext\n",
    "enable_profiler = False\n",
    "output_dir = \"tmp/llama-output\"\n",
    "\n",
    "# also tune-able\n",
    "config = {\n",
    "    'lora_config': lora_config,\n",
    "    'learning_rate': 2.5e-5,\n",
    "    'num_train_epochs': 2, # especially this one\n",
    "    'gradient_accumulation_steps': 1,\n",
    "    'per_device_train_batch_size': 1,\n",
    "    'gradient_checkpointing': True,\n",
    "}\n",
    "\n",
    "# Set up profiler, and connect to wandb.ai\n",
    "if enable_profiler:\n",
    "    wait, warmup, active, repeat = 1, 1, 2, 1\n",
    "    total_steps = (wait + warmup + active) * (1 + repeat)\n",
    "    schedule =  torch.profiler.schedule(wait=wait, warmup=warmup, active=active, repeat=repeat)\n",
    "    profiler = torch.profiler.profile(\n",
    "        schedule=schedule,\n",
    "        on_trace_ready=torch.profiler.tensorboard_trace_handler(f\"{output_dir}/logs/tensorboard\"),\n",
    "        record_shapes=True,\n",
    "        profile_memory=True,\n",
    "        with_stack=True)\n",
    "    \n",
    "    class ProfilerCallback(TrainerCallback):\n",
    "        def __init__(self, profiler):\n",
    "            self.profiler = profiler\n",
    "            \n",
    "        def on_step_end(self, *args, **kwargs):\n",
    "            self.profiler.step()\n",
    "\n",
    "    profiler_callback = ProfilerCallback(profiler)\n",
    "else:\n",
    "    profiler = nullcontext()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the Model, monitor on wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import default_data_collator, Trainer, TrainingArguments, DataCollatorForLanguageModeling\n",
    "\n",
    "# Define training args\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    overwrite_output_dir=True,\n",
    "    fp16=True,  # Use BF16 if available\n",
    "    # logging strategies\n",
    "    logging_dir=f\"{output_dir}/logs\",\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=10,\n",
    "    save_strategy=\"no\",\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=10,\n",
    "    optim=\"adamw_torch_fused\",\n",
    "    max_steps=total_steps if enable_profiler else -1,\n",
    "    **{k:v for k,v in config.items() if k != 'lora_config'}\n",
    ")\n",
    "\n",
    "with profiler:\n",
    "    # Create Trainer instance\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=tokenized_train_dataset,\n",
    "        eval_dataset= tokenized_validate_dataset,\n",
    "        data_collator=DataCollatorForLanguageModeling(tokenizer, mlm=False),\n",
    "        callbacks=[profiler_callback] if enable_profiler else [],\n",
    "    )\n",
    "    # Start training\n",
    "    trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wrap-up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge the weights, otherwise it will only save as lora weights, which llama.cpp does not support atm\n",
    "model = model.merge_and_unload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model\n",
    "model.save_pretrained(\"../models/13Bf_finetuned_02\")\n",
    "tokenizer.save_pretrained(\"../models/13Bf_finetuned_02\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;31mSignature:\u001b[0m\n",
      "\u001b[0mLlamaForCausalLM\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPathLike\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNoneType\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;34m*\u001b[0m\u001b[0mmodel_args\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mconfig\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtransformers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfiguration_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPretrainedConfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPathLike\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNoneType\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mcache_dir\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPathLike\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNoneType\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mignore_mismatched_sizes\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mforce_download\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mlocal_files_only\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mtoken\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNoneType\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mrevision\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'main'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0muse_safetensors\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mDocstring:\u001b[0m\n",
      "Instantiate a pretrained pytorch model from a pre-trained model configuration.\n",
      "\n",
      "The model is set in evaluation mode by default using `model.eval()` (Dropout modules are deactivated). To train\n",
      "the model, you should first set it back in training mode with `model.train()`.\n",
      "\n",
      "The warning *Weights from XXX not initialized from pretrained model* means that the weights of XXX do not come\n",
      "pretrained with the rest of the model. It is up to you to train those weights with a downstream fine-tuning\n",
      "task.\n",
      "\n",
      "The warning *Weights from XXX not used in YYY* means that the layer XXX is not used by YYY, therefore those\n",
      "weights are discarded.\n",
      "\n",
      "Parameters:\n",
      "    pretrained_model_name_or_path (`str` or `os.PathLike`, *optional*):\n",
      "        Can be either:\n",
      "\n",
      "            - A string, the *model id* of a pretrained model hosted inside a model repo on huggingface.co.\n",
      "              Valid model ids can be located at the root-level, like `bert-base-uncased`, or namespaced under a\n",
      "              user or organization name, like `dbmdz/bert-base-german-cased`.\n",
      "            - A path to a *directory* containing model weights saved using\n",
      "              [`~PreTrainedModel.save_pretrained`], e.g., `./my_model_directory/`.\n",
      "            - A path or url to a *tensorflow index checkpoint file* (e.g, `./tf_model/model.ckpt.index`). In\n",
      "              this case, `from_tf` should be set to `True` and a configuration object should be provided as\n",
      "              `config` argument. This loading path is slower than converting the TensorFlow checkpoint in a\n",
      "              PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.\n",
      "            - A path or url to a model folder containing a *flax checkpoint file* in *.msgpack* format (e.g,\n",
      "              `./flax_model/` containing `flax_model.msgpack`). In this case, `from_flax` should be set to\n",
      "              `True`.\n",
      "            - `None` if you are both providing the configuration and state dictionary (resp. with keyword\n",
      "              arguments `config` and `state_dict`).\n",
      "    model_args (sequence of positional arguments, *optional*):\n",
      "        All remaining positional arguments will be passed to the underlying model's `__init__` method.\n",
      "    config (`Union[PretrainedConfig, str, os.PathLike]`, *optional*):\n",
      "        Can be either:\n",
      "\n",
      "            - an instance of a class derived from [`PretrainedConfig`],\n",
      "            - a string or path valid as input to [`~PretrainedConfig.from_pretrained`].\n",
      "\n",
      "        Configuration for the model to use instead of an automatically loaded configuration. Configuration can\n",
      "        be automatically loaded when:\n",
      "\n",
      "            - The model is a model provided by the library (loaded with the *model id* string of a pretrained\n",
      "              model).\n",
      "            - The model was saved using [`~PreTrainedModel.save_pretrained`] and is reloaded by supplying the\n",
      "              save directory.\n",
      "            - The model is loaded by supplying a local directory as `pretrained_model_name_or_path` and a\n",
      "              configuration JSON file named *config.json* is found in the directory.\n",
      "    state_dict (`Dict[str, torch.Tensor]`, *optional*):\n",
      "        A state dictionary to use instead of a state dictionary loaded from saved weights file.\n",
      "\n",
      "        This option can be used if you want to create a model from a pretrained configuration but load your own\n",
      "        weights. In this case though, you should check if using [`~PreTrainedModel.save_pretrained`] and\n",
      "        [`~PreTrainedModel.from_pretrained`] is not a simpler option.\n",
      "    cache_dir (`Union[str, os.PathLike]`, *optional*):\n",
      "        Path to a directory in which a downloaded pretrained model configuration should be cached if the\n",
      "        standard cache should not be used.\n",
      "    from_tf (`bool`, *optional*, defaults to `False`):\n",
      "        Load the model weights from a TensorFlow checkpoint save file (see docstring of\n",
      "        `pretrained_model_name_or_path` argument).\n",
      "    from_flax (`bool`, *optional*, defaults to `False`):\n",
      "        Load the model weights from a Flax checkpoint save file (see docstring of\n",
      "        `pretrained_model_name_or_path` argument).\n",
      "    ignore_mismatched_sizes (`bool`, *optional*, defaults to `False`):\n",
      "        Whether or not to raise an error if some of the weights from the checkpoint do not have the same size\n",
      "        as the weights of the model (if for instance, you are instantiating a model with 10 labels from a\n",
      "        checkpoint with 3 labels).\n",
      "    force_download (`bool`, *optional*, defaults to `False`):\n",
      "        Whether or not to force the (re-)download of the model weights and configuration files, overriding the\n",
      "        cached versions if they exist.\n",
      "    resume_download (`bool`, *optional*, defaults to `False`):\n",
      "        Whether or not to delete incompletely received files. Will attempt to resume the download if such a\n",
      "        file exists.\n",
      "    proxies (`Dict[str, str]`, *optional*):\n",
      "        A dictionary of proxy servers to use by protocol or endpoint, e.g., `{'http': 'foo.bar:3128',\n",
      "        'http://hostname': 'foo.bar:4012'}`. The proxies are used on each request.\n",
      "    output_loading_info(`bool`, *optional*, defaults to `False`):\n",
      "        Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.\n",
      "    local_files_only(`bool`, *optional*, defaults to `False`):\n",
      "        Whether or not to only look at local files (i.e., do not try to download the model).\n",
      "    token (`str` or `bool`, *optional*):\n",
      "        The token to use as HTTP bearer authorization for remote files. If `True`, or not specified, will use\n",
      "        the token generated when running `huggingface-cli login` (stored in `~/.huggingface`).\n",
      "    revision (`str`, *optional*, defaults to `\"main\"`):\n",
      "        The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a\n",
      "        git-based system for storing models and other artifacts on huggingface.co, so `revision` can be any\n",
      "        identifier allowed by git.\n",
      "\n",
      "        <Tip>\n",
      "\n",
      "        To test a pull request you made on the Hub, you can pass `revision=\"refs/pr/<pr_number>\".\n",
      "\n",
      "        </Tip>\n",
      "\n",
      "    mirror (`str`, *optional*):\n",
      "        Mirror source to accelerate downloads in China. If you are from China and have an accessibility\n",
      "        problem, you can set this option to resolve it. Note that we do not guarantee the timeliness or safety.\n",
      "        Please refer to the mirror site for more information.\n",
      "    _fast_init(`bool`, *optional*, defaults to `True`):\n",
      "        Whether or not to disable fast initialization.\n",
      "\n",
      "        <Tip warning={true}>\n",
      "\n",
      "        One should only disable *_fast_init* to ensure backwards compatibility with `transformers.__version__ <\n",
      "        4.6.0` for seeded model initialization. This argument will be removed at the next major version. See\n",
      "        [pull request 11471](https://github.com/huggingface/transformers/pull/11471) for more information.\n",
      "\n",
      "        </Tip>\n",
      "\n",
      "    > Parameters for big model inference\n",
      "\n",
      "    low_cpu_mem_usage(`bool`, *optional*):\n",
      "        Tries to not use more than 1x model size in CPU memory (including peak memory) while loading the model.\n",
      "        This is an experimental feature and a subject to change at any moment.\n",
      "    torch_dtype (`str` or `torch.dtype`, *optional*):\n",
      "        Override the default `torch.dtype` and load the model under a specific `dtype`. The different options\n",
      "        are:\n",
      "\n",
      "        1. `torch.float16` or `torch.bfloat16` or `torch.float`: load in a specified\n",
      "          `dtype`, ignoring the model's `config.torch_dtype` if one exists. If not specified\n",
      "          - the model will get loaded in `torch.float` (fp32).\n",
      "\n",
      "        2. `\"auto\"` - A `torch_dtype` entry in the `config.json` file of the model will be\n",
      "          attempted to be used. If this entry isn't found then next check the `dtype` of the first weight in\n",
      "          the checkpoint that's of a floating point type and use that as `dtype`. This will load the model\n",
      "          using the `dtype` it was saved in at the end of the training. It can't be used as an indicator of how\n",
      "          the model was trained. Since it could be trained in one of half precision dtypes, but saved in fp32.\n",
      "\n",
      "        <Tip>\n",
      "\n",
      "        For some models the `dtype` they were trained in is unknown - you may try to check the model's paper or\n",
      "        reach out to the authors and ask them to add this information to the model's card and to insert the\n",
      "        `torch_dtype` entry in `config.json` on the hub.\n",
      "\n",
      "        </Tip>\n",
      "\n",
      "    device_map (`str` or `Dict[str, Union[int, str, torch.device]]` or `int` or `torch.device`, *optional*):\n",
      "        A map that specifies where each submodule should go. It doesn't need to be refined to each\n",
      "        parameter/buffer name, once a given module name is inside, every submodule of it will be sent to the\n",
      "        same device. If we only pass the device (*e.g.*, `\"cpu\"`, `\"cuda:1\"`, `\"mps\"`, or a GPU ordinal rank\n",
      "        like `1`) on which the model will be allocated, the device map will map the entire model to this\n",
      "        device. Passing `device_map = 0` means put the whole model on GPU 0.\n",
      "\n",
      "        To have Accelerate compute the most optimized `device_map` automatically, set `device_map=\"auto\"`. For\n",
      "        more information about each option see [designing a device\n",
      "        map](https://hf.co/docs/accelerate/main/en/usage_guides/big_modeling#designing-a-device-map).\n",
      "    max_memory (`Dict`, *optional*):\n",
      "        A dictionary device identifier to maximum memory. Will default to the maximum memory available for each\n",
      "        GPU and the available CPU RAM if unset.\n",
      "    offload_folder (`str` or `os.PathLike`, *optional*):\n",
      "        If the `device_map` contains any value `\"disk\"`, the folder where we will offload weights.\n",
      "    offload_state_dict (`bool`, *optional*):\n",
      "        If `True`, will temporarily offload the CPU state dict to the hard drive to avoid getting out of CPU\n",
      "        RAM if the weight of the CPU state dict + the biggest shard of the checkpoint does not fit. Defaults to\n",
      "        `True` when there is some disk offload.\n",
      "    load_in_8bit (`bool`, *optional*, defaults to `False`):\n",
      "        If `True`, will convert the loaded model into mixed-8bit quantized model. To use this feature please\n",
      "        install `bitsandbytes` (`pip install -U bitsandbytes`).\n",
      "    load_in_4bit (`bool`, *optional*, defaults to `False`):\n",
      "        If `True`, will convert the loaded model into 4bit precision quantized model. To use this feature\n",
      "        install the latest version of `bitsandbytes` (`pip install -U bitsandbytes`).\n",
      "    quantization_config (`Union[QuantizationConfigMixin,Dict]`, *optional*):\n",
      "        A dictionary of configuration parameters or a QuantizationConfigMixin object for quantization (e.g\n",
      "        bitsandbytes, gptq)\n",
      "    subfolder (`str`, *optional*, defaults to `\"\"`):\n",
      "        In case the relevant files are located inside a subfolder of the model repo on huggingface.co, you can\n",
      "        specify the folder name here.\n",
      "    variant (`str`, *optional*):\n",
      "        If specified load weights from `variant` filename, *e.g.* pytorch_model.<variant>.bin. `variant` is\n",
      "        ignored when using `from_tf` or `from_flax`.\n",
      "    use_safetensors (`bool`, *optional*, defaults to `None`):\n",
      "        Whether or not to use `safetensors` checkpoints. Defaults to `None`. If not specified and `safetensors`\n",
      "        is not installed, it will be set to `False`.\n",
      "\n",
      "    kwargs (remaining dictionary of keyword arguments, *optional*):\n",
      "        Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,\n",
      "        `output_attentions=True`). Behaves differently depending on whether a `config` is provided or\n",
      "        automatically loaded:\n",
      "\n",
      "            - If a configuration is provided with `config`, `**kwargs` will be directly passed to the\n",
      "              underlying model's `__init__` method (we assume all relevant updates to the configuration have\n",
      "              already been done)\n",
      "            - If a configuration is not provided, `kwargs` will be first passed to the configuration class\n",
      "              initialization function ([`~PretrainedConfig.from_pretrained`]). Each key of `kwargs` that\n",
      "              corresponds to a configuration attribute will be used to override said attribute with the\n",
      "              supplied `kwargs` value. Remaining keys that do not correspond to any configuration attribute\n",
      "              will be passed to the underlying model's `__init__` function.\n",
      "\n",
      "<Tip>\n",
      "\n",
      "Activate the special [\"offline-mode\"](https://huggingface.co/transformers/installation.html#offline-mode) to\n",
      "use this method in a firewalled environment.\n",
      "\n",
      "</Tip>\n",
      "\n",
      "Examples:\n",
      "\n",
      "```python\n",
      ">>> from transformers import BertConfig, BertModel\n",
      "\n",
      ">>> # Download model and configuration from huggingface.co and cache.\n",
      ">>> model = BertModel.from_pretrained(\"bert-base-uncased\")\n",
      ">>> # Model was saved using *save_pretrained('./test/saved_model/')* (for example purposes, not runnable).\n",
      ">>> model = BertModel.from_pretrained(\"./test/saved_model/\")\n",
      ">>> # Update configuration during loading.\n",
      ">>> model = BertModel.from_pretrained(\"bert-base-uncased\", output_attentions=True)\n",
      ">>> assert model.config.output_attentions == True\n",
      ">>> # Loading from a TF checkpoint file instead of a PyTorch model (slower, for example purposes, not runnable).\n",
      ">>> config = BertConfig.from_json_file(\"./tf_model/my_tf_model_config.json\")\n",
      ">>> model = BertModel.from_pretrained(\"./tf_model/my_tf_checkpoint.ckpt.index\", from_tf=True, config=config)\n",
      ">>> # Loading from a Flax checkpoint file instead of a PyTorch model (slower)\n",
      ">>> model = BertModel.from_pretrained(\"bert-base-uncased\", from_flax=True)\n",
      "```\n",
      "\n",
      "* `low_cpu_mem_usage` algorithm:\n",
      "\n",
      "This is an experimental function that loads the model using ~1x model size CPU memory\n",
      "\n",
      "Here is how it works:\n",
      "\n",
      "1. save which state_dict keys we have\n",
      "2. drop state_dict before the model is created, since the latter takes 1x model size CPU memory\n",
      "3. after the model has been instantiated switch to the meta device all params/buffers that\n",
      "are going to be replaced from the loaded state_dict\n",
      "4. load state_dict 2nd time\n",
      "5. replace the params/buffers from the state_dict\n",
      "\n",
      "Currently, it can't handle deepspeed ZeRO stage 3 and ignores loading errors\n",
      "\u001b[0;31mFile:\u001b[0m      ~/miniconda3/envs/llama_env/lib/python3.9/site-packages/transformers/modeling_utils.py\n",
      "\u001b[0;31mType:\u001b[0m      method"
     ]
    }
   ],
   "source": [
    "?LlamaForCausalLM.from_pretrained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LlamaForCausalLM.from_pretrained(\"../models/13Bf_finetuned_02\", )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<<SYS>>You are a helpful Assistant, you take course catalogue raw text extract and return a clean JSON object list. If available, the JSON objects should contain for each course in the text a \"Number\", \"Title\", \"Description\", \"Prerequisites\", and \"Credits\". There can be multiple Course Numbers include all of them. If the text is not a course description, return '[]'.<</SYS>>\n",
      "\n",
      "[INST]Text: oselective and\n",
      "enantioselective processes. Prerequisite: CHEM 241.\n",
      "CHEM 260. Advanced Physical Chemistry. 3 Credits.\n",
      "Builds on the concepts from Introductory Physical Chemistry\n",
      "(CHEM 165). The three major areas of quantum chemistry,\n",
      "thermodynamics, and kinetics are extended in greater depth, and at\n",
      "a higher level of mathematical rigor. Prerequisite: CHEM 165. Co-\n",
      "requisites: CHEM 167 or MATH 121.\n",
      "CHEM 267. Topics in Physical Chemistry. 1-3 Credits.\n",
      "Selected topics of current interest in physical chemistry. See Schedule\n",
      "of Courses for specific titles. May be repeated for credit with different\n",
      "content. Prerequisite: CHEM 260.\n",
      "CHEM 282. Senior Seminar. 1 Credit.\n",
      "Oral and written presentation of a subject of current chemical\n",
      "interest. Pre/Co-requisite: Audit of CHEM 381.\n",
      "CHEM 285. Special Topics. 1-3 Credits.\n",
      "Selected topics of current interest that do not fall into one of the\n",
      "traditional areas of chemistry.\n",
      "CHEM 286. Special Topics. 1-3 Credits.\n",
      "Selected topics of current interest that do not fall into one of the\n",
      "traditional areas of chemistry.\n",
      "CHEM 290. Internship. 1-18 Credits.\n",
      "On-site supervised work experience combined with a structured\n",
      "academic learning plan directed by a faculty member or a faculty-staff\n",
      "team in which a faculty member is the instructor of record, for which\n",
      "academic credit is Offered at department discretion.\n",
      "CHEM 291. Undergraduate Research. 1-18 Credits.\n",
      "Undergraduate students work on research projects under the\n",
      "supervision of a faculty member, for which credit is awarded. Offered\n",
      "at department discretion. Prerequisite: Departmental permission.\n",
      "CHEM 292. Independent Study. 1-18 Credits.\n",
      "A course which is tailored to fit the interests of a specific student,\n",
      "which occurs outside the traditional classroom/laboratory setting\n",
      "under the supervision of a faculty member, for which credit is\n",
      "awarded. Offered at department discretion.\n",
      "CHEM 293. Teaching Assistantship. 1-3 Credits.\n",
      "Undergraduate student service as a teaching assistant, usually in\n",
      "an introductory-level course in the discipline, for which credit is\n",
      "awarded. Offered at department discretion.\n",
      "53\n",
      "THE UNIVERSITY OF VERMONT UNDERGRADUATE CATALOGUE 2020-2021\n",
      "CHEM 295. Advanced Special Topics. 1-18 Credits.\n",
      "See Schedule of Courses for specific titles.\n",
      "CHEM 296. Advanced Special Topics. 1-18 Credits.\n",
      "See Schedule of Courses for specific titles.\n",
      "CHINESE (CHIN)\n",
      "Courses\n",
      "CHIN 001. Elementary Chinese I. 4 Credits.\n",
      "A study of Mandarin Chinese designed to give students the\n",
      "fundamentals of the sound and writing systems for developing\n",
      "modern Chinese communicative skills. No prior knowledge expected.\n",
      "CHIN 002. Elementary Chinese II. 4 Credits.\n",
      "A continuation of CHIN 001 designed to give students basic\n",
      "Chinese grammar and vocabulary for daily communication purposes.\n",
      "Prerequisite: CHIN 001 or equivalent.\n",
      "CHIN 020. Chinese Characters. 1 Credit.\n",
      "Understand the Chinese writing system and learn to recognize and\n",
      "write basic Chinese characters.\n",
      "CHIN 051. Intermediate Chinese I. 4 Credits.[/INST]\n",
      "\n",
      "[\n",
      "    {\n",
      "        \"Number\": [\n",
      "            260\n",
      "        ],\n",
      "        \"Title\": \"Advanced Physical Chemistry\",\n",
      "        \"Description\": \"Builds on the concepts from Introductory Physical Chemistry (CHEM 165). The three major areas of quantum chemistry, thermodynamics, and kinetics are extended in greater depth, and at a higher level of mathematical rigor\",\n",
      "        \"Prerequisites\": \"CHEM 165\",\n",
      "        \"Credits\": \"3\"\n",
      "    },\n",
      "    {\n",
      "        \"Number\": [\n",
      "            267\n",
      "        ],\n",
      "        \"Title\": \"Topics in Physical Chemistry\",\n",
      "        \"Description\": \"Selected topics of current interest in physical chemistry. See Schedule of Courses for specific titles. May be repeated for credit with different content\",\n",
      "        \"Prerequisites\": \"CHEM 260\",\n",
      "        \"Credits\": \"1-3\"\n",
      "    },\n",
      "    {\n",
      "        \"Number\": [\n",
      "            282\n",
      "        ],\n",
      "        \"Title\": \"Senior Seminar\",\n",
      "        \"Description\": \"Oral and written presentation of a subject of current interest\",\n",
      "        \"Prerequisites\": \"Audit of CHEM 381\",\n",
      "        \"Credits\": \"1\"\n",
      "    },\n",
      "    {\n",
      "        \"Number\": [\n",
      "            285\n",
      "        ],\n",
      "        \"Title\": \"Special Topics\",\n",
      "        \"Description\": \"Selected topics of current interest that do not fall into one of the traditional areas of chemistry\",\n",
      "        \"Prerequisites\": \"\",\n",
      "        \"Credits\": \"1-3\"\n",
      "    },\n",
      "    {\n",
      "        \"Number\": [\n",
      "            286\n",
      "        ],\n",
      "        \"Title\": \"Special Topics\",\n",
      "        \"Description\": \"Selected topics of current interest that do not fall into one of the traditional areas of chemistry\",\n",
      "        \"Prerequisites\": \"\",\n",
      "        \"Credits\": \"1-3\"\n",
      "    },\n",
      "    {\n",
      "        \"Number\": [\n",
      "            290\n",
      "        ],\n",
      "        \"Title\": \"Internship\",\n",
      "        \"Description\": \"On-site supervised work experience combined with a structured academic learning plan directed by a faculty member or a faculty-staff team in which a faculty member is the instructor of record, for which academic credit is Offered at department discretion\",\n",
      "        \"Prerequisites\": \"\",\n",
      "        \"Credits\": \"1-18\"\n",
      "    },\n",
      "    {\n",
      "        \"Number\": [\n",
      "            291\n",
      "        ],\n",
      "        \"Title\": \"Undergraduate Research\",\n",
      "        \"Description\": \"Undergraduate students work on research projects under the supervision of a faculty member, for which credit is awarded. Offered at department discretion\",\n",
      "        \"Prerequisites\": \"Departmental permission\",\n",
      "        \"Credits\": \"1-18\"\n",
      "    },\n",
      "    {\n",
      "        \"Number\": [\n",
      "            292\n",
      "        ],\n",
      "        \"Title\": \"Independent Study\",\n",
      "        \"Description\": \"A course which is tailored to fit the interests of a specific student, which occurs outside the traditional classroomlaboratory setting under the supervision of a faculty member, for which credit is awarded. Offered at department discretion\",\n",
      "        \"Prerequisites\": \"\",\n",
      "        \"Credits\": \"1-18\"\n",
      "    },\n",
      "    {\n",
      "        \"Number\": [\n",
      "            293\n",
      "        ],\n",
      "        \"Title\": \"Teaching Assistantship\",\n",
      "        \"Description\": \"Undergraduate student service as a teaching assistant, usually in an introductory-level course in the discipline, for which credit is awarded. Offered at department discretion\",\n",
      "        \"Prerequisites\": \"\",\n",
      "        \"Credits\": \"1-3\"\n",
      "    },\n",
      "    {\n",
      "        \"Number\": [\n",
      "            295\n",
      "        ],\n",
      "        \"Title\": \"Advanced Special Topics\",\n",
      "        \"Description\": \"See Schedule of Courses for specific titles\",\n",
      "        \"Prerequisites\": \"\",\n",
      "        \"Credits\": \"1-18\"\n",
      "    },\n",
      "    {\n",
      "        \"Number\": [\n",
      "            296\n",
      "        ],\n",
      "        \"Title\": \"Advanced Special Topics\",\n",
      "        \"Description\": \"See Schedule of Courses for specific titles\",\n",
      "        \"Prerequisites\": \"\",\n",
      "        \"Credits\": \"1-18\"\n",
      "    },\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the new model\n",
    "eval_prompt = f\"\"\"<<SYS>>{sys_prompt}<</SYS>>\n",
    "\n",
    "[INST]Text: {tokenized_validate_dataset[10]['input']}[/INST]\n",
    "\"\"\"\n",
    "\n",
    "model_input = tokenizer(eval_prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    print(tokenizer.decode(model.generate(**model_input, max_new_tokens=1000)[0], skip_special_tokens=True))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
