`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...
/users/j/s/jstonge1/miniconda3/envs/llama_env/lib/python3.9/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
Tue Jan 16 09:49:11 2024
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.104.12             Driver Version: 535.104.12   CUDA Version: 12.2     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  Tesla V100-SXM2-32GB           On  | 00000000:65:00.0 Off |                    0 |
| N/A   35C    P0              58W / 300W |  29563MiB / 32768MiB |      0%   E. Process |
|                                         |                      |                  N/A |
+-----------------------------------------+----------------------+----------------------+
|   1  Tesla V100-SXM2-32GB           On  | 00000000:66:00.0 Off |                    0 |
| N/A   34C    P0              56W / 300W |  30969MiB / 32768MiB |      0%   E. Process |
|                                         |                      |                  N/A |
+-----------------------------------------+----------------------+----------------------+
|   2  Tesla V100-SXM2-32GB           On  | 00000000:B6:00.0 Off |                    0 |
| N/A   36C    P0              58W / 300W |  16865MiB / 32768MiB |      0%   E. Process |
|                                         |                      |                  N/A |
+-----------------------------------------+----------------------+----------------------+
|   3  Tesla V100-SXM2-32GB           On  | 00000000:B7:00.0 Off |                    0 |
| N/A   35C    P0              55W / 300W |  14405MiB / 32768MiB |      0%   E. Process |
|                                         |                      |                  N/A |
+-----------------------------------------+----------------------+----------------------+
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
|    0   N/A  N/A    102963      C   ...iniconda3/envs/llama_env/bin/python    29558MiB |
|    1   N/A  N/A    102963      C   ...iniconda3/envs/llama_env/bin/python    30964MiB |
|    2   N/A  N/A    102963      C   ...iniconda3/envs/llama_env/bin/python    16860MiB |
|    3   N/A  N/A    102963      C   ...iniconda3/envs/llama_env/bin/python    14402MiB |
